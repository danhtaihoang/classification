{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "This Jupyter Noterbook helps us to convert binary attribute(s) to +/-1, categorical attributes(s) to onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data which were cleaned from the `data cleaning` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "[['1864' '0' '1' '0']\n",
      " ['1864' '0' '1' '1']\n",
      " ['1864' '0' '1' '0']\n",
      " ['1864' '0' '1' '0']\n",
      " ['1864' '0' '1' '1']\n",
      " ['1864' '0' '1' '1']\n",
      " ['1864' '0' '1' '0']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '0']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '0']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '1']\n",
      " ['1864' '0' '2' '0']\n",
      " ['1866' '0' '1' '1']\n",
      " ['1866' '0' '1' '0']\n",
      " ['1866' '0' '1' '0']\n",
      " ['1866' '0' '1' '1']\n",
      " ['1866' '0' '1' '1']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1866' '0' '2' '1']\n",
      " ['1866' '0' '2' '1']\n",
      " ['1866' '0' '2' '1']\n",
      " ['1866' '0' '2' '1']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1866' '0' '2' '1']\n",
      " ['1866' '0' '2' '1']\n",
      " ['1866' '0' '2' '0']\n",
      " ['1867' '1' '1' '1']\n",
      " ['1867' '1' '1' '1']\n",
      " ['1867' '1' '1' '1']\n",
      " ['1867' '1' '2' '1']\n",
      " ['1867' '1' '2' '1']\n",
      " ['1867' '1' '2' '1']\n",
      " ['1867' '1' '2' '1']\n",
      " ['1868' '1' '1' '1']\n",
      " ['1868' '1' '1' '1']\n",
      " ['1868' '1' '1' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '0']\n",
      " ['1868' '1' '2' '0']\n",
      " ['1868' '1' '2' '0']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1868' '1' '2' '1']\n",
      " ['1869' '1' '1' '1']\n",
      " ['1869' '1' '1' '1']\n",
      " ['1869' '1' '1' '1']\n",
      " ['1869' '1' '1' '1']\n",
      " ['1869' '1' '1' '1']\n",
      " ['1869' '1' '1' '0']\n",
      " ['1869' '1' '2' '0']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '0']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '1']\n",
      " ['1869' '1' '2' '1']]\n"
     ]
    }
   ],
   "source": [
    "Xy = np.loadtxt('amputation_cleaned.dat', dtype = 'str')\n",
    "\n",
    "print(Xy.shape)\n",
    "print(Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find number of unique value for each column, to have an idea about which variables are continuous, which variables are binary, category. It depends on data, however it is likely that nu = 2 --> binary; nu = 3 or 4: --> category, n > 4: continuous. Of course, we have to see data in detail as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of uniques of each variable:\n",
      "[5 2 2]\n"
     ]
    }
   ],
   "source": [
    "X = Xy[:,:-1]\n",
    "l,n = X.shape\n",
    "nu = np.array([len(np.unique(X[:,i])) for i in range(n)])\n",
    "print('number of uniques of each variable:')\n",
    "print(nu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define variable type, 1: continuous, 2: binary, 3: category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "variable_type  = np.ones(n) # continuous\n",
    "variable_type[1:] = 2 # binary\n",
    "\n",
    "\n",
    "print(variable_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert binary to +/-1, category to onehot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary_and_category(x,variable_type):\n",
    "    \"\"\"\n",
    "    convert binary to +-1, category to one hot; remain continuous.\n",
    "    \"\"\"\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "\n",
    "    # create 2 initial columns\n",
    "    x_new = np.zeros((x.shape[0],2))\n",
    "\n",
    "    for i,i_type in enumerate(variable_type):\n",
    "        if i_type == 1: # continuous\n",
    "            x_new = np.hstack((x_new,x[:,i][:,np.newaxis]))\n",
    "        elif i_type == 2: # binary\n",
    "            unique_value = np.unique(x[:,i])\n",
    "            x1 = np.array([-1. if value == unique_value[0] else 1. for value in x[:,i]])        \n",
    "            x_new = np.hstack((x_new,x1[:,np.newaxis]))\n",
    "        else: # category      \n",
    "            x1 = onehot_encoder.fit_transform(x[:,i].reshape(-1,1))\n",
    "            x_new = np.hstack((x_new,x1))        \n",
    "\n",
    "    # drop the 2 initial column\n",
    "    x_new = x_new[:,2:]\n",
    "    \n",
    "    return x_new.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 3)\n",
      "[[ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.864e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00 -1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.866e+03 -1.000e+00  1.000e+00]\n",
      " [ 1.867e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.867e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.867e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.867e+03  1.000e+00  1.000e+00]\n",
      " [ 1.867e+03  1.000e+00  1.000e+00]\n",
      " [ 1.867e+03  1.000e+00  1.000e+00]\n",
      " [ 1.867e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.868e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.868e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.868e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.869e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.869e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.869e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.869e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.869e+03  1.000e+00 -1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]\n",
      " [ 1.869e+03  1.000e+00  1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# convert X\n",
    "X_new = convert_binary_and_category(X,variable_type)\n",
    "\n",
    "print(X_new.shape)\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([22, 53]))\n"
     ]
    }
   ],
   "source": [
    "## target\n",
    "y = Xy[:,-1].astype(float)\n",
    "\n",
    "print(np.unique(y,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([22, 53]))\n"
     ]
    }
   ],
   "source": [
    "y_new = y\n",
    "# convert taget to 0 and 1\n",
    "#y_new = np.ones(y.shape[0])\n",
    "\n",
    "\n",
    "print(np.unique(y_new,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine X and y\n",
    "Xy_new = np.hstack((X_new,y_new[:,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('amputation_processed.dat',Xy_new,fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
